# Predicting retention


## Funnel

What is the per-answer retention rate funnel?

    >>> from answer_retention import *

Using the data sample mentioned in [README.md](README.md).

Summarize funnel in a test sample file:

    >>> funnel_args = '--funnel test/answers_sample_small.csv'.split()
    >>> print(retention_args(funnel_args))
    test/answers_sample_small.funnel.csv
    answer_count,retention_count,step_retention,total_retention
    1,2,1.000,1.000
    2,2,1.000,1.000
    3,1,0.500,0.500
    <BLANKLINE>

The retention count depends on counting frequencies less than or equal to a value.

    >>> reverse_cumulative_frequency([1, 4, 2, 2, 8])
    array([5, 4, 2, 2, 1, 1, 1, 1])

    >>> reverse_cumulative_frequency([2, 3])
    array([2, 2, 1])

Total retention rate depends on retention rate.

    >>> retention_counts = [5, 4, 2, 2, 1, 1, 1, 1]
    >>> retention_rates(retention_counts)
    [1.0, 0.8, 0.4, 0.4, 0.2, 0.2, 0.2, 0.2]
    >>> retention_steps(retention_counts)
    [1.0, 0.8, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0]



## Bottleneck

Where is the bottleneck in the funnel of answers per user?

The funnel step retention shows a drop off after the first and each tenth answer.

Example from:

    python answer_retention.py --funnel data/answers.csv

    answer_count,retention_count,step_retention,total_retention
    1,18847,1.000,1.000
    2,16558,0.879,0.879
    3,15132,0.914,0.803
    4,14151,0.935,0.751
    5,13388,0.946,0.710
    6,12791,0.955,0.679
    7,12317,0.963,0.654
    8,11968,0.972,0.635
    9,11668,0.975,0.619
    10,11353,0.973,0.602
    11,8408,0.741,0.446
    12,7971,0.948,0.423
    13,7637,0.958,0.405

The authors wrote the tenth answer is an unadapted difficulty to assess learning.

Or perhaps there is a session break every ten answers.



## Predicting step retention

What features of the answer predict answering the next answer?

### Extending features

I marked each student's last answer by having no future answer.

Also marked the answer number and modulo-ten of the answer number.

Above each 10th answer predicted a dropoff.

    >>> feature_args = '--feature test/answers_sample_small.csv'.split()
    >>> print(retention_args(feature_args))
    test/answers_sample_small.feature.csv
    id,time,item,student,response_time,correct,answer,answer_expected,log,random,future_answers,nth,is_10th,is_future_row
    273951,2016-04-17 14:12:09,38,33480,57276,0,14,13,"{""client_meta"": [[38008, ""13 selected""], [39224, ""unselected""], [49135, ""1 selected""], [55376, ""14 selected""], [57276, ""finished""]], ""device"": ""desktop""}",0,2,1,False,True
    273952,2016-04-17 14:12:32,51,33480,17068,0,3,17,"{""client_meta"": [[11382, ""soft-keyboard:3""], [11382, ""3""], [17068, ""finished""]], ""device"": ""desktop""}",0,1,2,False,True
    273953,2016-04-17 14:12:45,685,33481,19878,0,95,85,"{""client_meta"": [[13513, ""9""], [18550, ""95""], [19878, ""finished""]], ""device"": ""desktop""}",0,1,1,False,True
    273954,2016-04-17 14:13:03,427,33480,15139,1,20,20,"{""client_meta"": [[12145, ""2""], [12399, ""20""], [15139, ""finished""]], ""device"": ""desktop""}",0,0,3,False,False
    273955,2016-04-17 14:13:04,148,33481,12359,1,10,10,"{""client_meta"": [[10680, ""1""], [11409, ""10""], [12359, ""finished""]], ""device"": ""desktop""}",0,0,2,False,False
    <BLANKLINE>


### Predicting future answer from answer features

The decision tree claimed a score of 0.95, from the features in order of biggest score first:

    is_10th
    correct

10 question classified the most, followed by being correct.

Roughly the decision tree proportions are:

* 10th answer:
 * if wrong:  1/4 quit.
 * if correct: 1/8 quit.

* Other answers:
 * if wrong: 1/13 quit.
 * if correct:  1/30 quit.

The PDF describes those standardized features in the original order, listed below.

However, in the PDF, there is still a lot of samples of results responding or not, so I doubt this score indicates the raw reliability of the prediction.

Answer correctness seems to correlate to about a 5% difference in answering again, with twice as many answers being their last when wrong.  In the PDF of the decision tree, there is around 95% chance of another answer when correct (among 196727 answers), and around 90% chance when wrong (among 28580 answers).

Prediciton generated by this command:

    python answer_retention.py --feature --predict data/answers.csv

Output:

    features_classes: features: ['id', 'item', 'response_time', 'correct', 'answer', 'answer_expected', 'random', 'nth', 'is_10th']
        scores array([  4.49690660e+00,   1.35594392e+02,   2.83205226e+00,
             1.60158287e+03,              nan,   4.25063970e+02,
             7.51870947e+00,   1.21954860e+03,   8.64552065e+03])
        p-values array([  3.39568816e-002,   2.47809517e-031,   9.24010441e-002,
             0.00000000e+000,               nan,   2.17608413e-094,
             6.10643014e-003,   9.26143277e-267,   0.00000000e+000])
        support array([False, False, False,  True, False, False, False, False,  True], dtype=bool)
    Decision tree graphed in file 'data/answers.predict.pdf'
    Decision Tree score 0.95 features 2 pdf data/answers.predict.pdf


#### Test examples

    >>> predict_args = '--predict --feature test/answers_sample_small.csv'.split()
    >>> print(retention_args(predict_args)) #doctest: +ELLIPSIS
    features_classes: features: ['id', 'item', 'response_time', 'correct', 'answer', 'answer_expected', 'nth']
    ...
    Decision tree graphed in file 'test/answers_sample_small.predict.pdf'
    Decision Tree score 1.0 features 2 pdf test/answers_sample_small.predict.pdf


    >>> predict_corrupt_args = '--predict --feature test/answers_sample.csv'.split()
    >>> print(retention_args(predict_corrupt_args)) #doctest: +ELLIPSIS
    features_classes: features: ['id', 'item', 'response_time', 'correct', 'answer', 'answer_expected', 'nth', 'is_10th']
    ...
    Decision tree graphed in file 'test/answers_sample.predict.pdf'
    Decision Tree score 0.95 features 2 pdf test/answers_sample.predict.pdf

Drop answers that are not a numeric format.
For example, this test sample has text as last answer instead of an integer.

    >>> parse_answers('test/answers_sample.csv')['answer'].tail(2)
    1022    18.0
    1023    17.0
    Name: answer, dtype: float64

Other classifier indexes can also be scored:

    >>> predict_corrupt_args = '--classifier 2 --predict --feature test/answers_sample.csv'.split()
    >>> print(retention_args(predict_corrupt_args)) #doctest: +ELLIPSIS
    features_classes: features: ['id', 'item', 'response_time', 'correct', 'answer', 'answer_expected', 'nth', 'is_10th']
    ...
    Gaussian Process score 0.95 features 2


## Predict second only

Here is an example of only predicting second row per user.

    >>> drop_2 = read_csv('test/custom_column.csv')
    >>> drop_nth(drop_2, 1)
    >>> drop_2
           id            timestamp  item   user  response_time  correct  got  \
    0  273951  2016-04-17 14:12:09    38  33480          57276        0   14   
    4  273955  2016-04-17 14:13:04   148  33482          12359        1   10   
    5  273955  2016-04-17 14:13:04    38  33483          12359        1   10   
    <BLANKLINE>
       expected  nth  
    0        13    1  
    4        10    1  
    5        10    1  

    >>> predict_second_args = 'test/custom_column.csv --predict --feature --max_nth 1 --user_column user --ignore_columns timestamp,got'.split()
    >>> print(retention_args(predict_second_args)) #doctest: +ELLIPSIS
    features_classes: features: ['id', 'item', 'response_time', 'correct', 'expected']
        ...
    Decision tree graphed in file 'test/custom_column.predict.pdf'
    Decision Tree score 0.5 features 2 pdf test/custom_column.predict.pdf


## Custom columns

Here is an example of a custom user column and columns to ignore from the prediction.

    >>> predict_custom_args = '--predict --feature test/custom_column.csv --user_column user --ignore_columns timestamp,got'.split()
    >>> print(retention_args(predict_custom_args)) #doctest: +ELLIPSIS
    features_classes: features: ['id', 'item', 'response_time', 'correct', 'expected']
        ...
    Decision tree graphed in file 'test/custom_column.predict.pdf'
    Decision Tree score 0.5 features 2 pdf test/custom_column.predict.pdf

## Future directions

How can insight be formatted in terms of original data?
Currently it is listed by column indexes.

Is this more stable when filtering out players who started in the last week of the sample?

What is second week retention rate?  What is second day in first week retention rate?

How well does 10 answer accuracy or any other factors predict second week retention?  Second day in first week retention?
